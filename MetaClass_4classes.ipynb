{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central questions: can classifiers be trained using neuroimaging meta-analysis data (i.e., coordinates, modeled activation maps, etc.). \n",
    "Goals: [1] train classifier using coordinates/modeled activation maps, [2]..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime; today = datetime.date.today()\n",
    "from nimare.io import convert_sleuth_to_dataset\n",
    "from nimare.meta.cbma.kernel import ALEKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select directories and files structure.\n",
    "input_prefix = str(today)\n",
    "output_prefix = str(today)\n",
    "in_dir = 'constructs'\n",
    "out_dir = 'out'\n",
    "test_dir = 'neurosynth'\n",
    "# paths = glob.glob(\"constructs/*.txt\")\n",
    "paths=['constructs/saccades.txt',\n",
    "      'constructs/tmind.txt',\n",
    "      'constructs/pain.txt',\n",
    "      'constructs/counting.txt']\n",
    "print('constructs to model = {0}'.format(len(paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert coordinates to nimare dataset. DOES NOT WORK!\n",
    "datas = {}\n",
    "for path in paths:\n",
    "    print(path)\n",
    "    datas[path[len(in_dir) + 1:-4]] = convert_sleuth_to_dataset(path)\n",
    "datas.keys()  # Confirm construct keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MA maps making...\\t\\t@{0}'.format(str(datetime.datetime.now())))\n",
    "ma_maps_arrs = {}\n",
    "for data in datas.keys():\n",
    "    kern = ALEKernel()\n",
    "    ma_maps = kern.transform(datas[data])  # Compute MA maps (len = ???)\n",
    "    ma_maps_arrs[data] = []\n",
    "    for i in np.arange(0, len(ma_maps)):\n",
    "        ma_maps_arrs[data].append(np.ravel(ma_maps[i].get_data(), order='C'))\n",
    "    labels = pd.DataFrame(index=datas[data].ids)\n",
    "print('MA maps done!\\t\\t\\t@{0}'.format(str(datetime.datetime.now())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(datas.keys())\n",
    "dataframes = {}\n",
    "key = {}\n",
    "for i in np.arange(0, len(keys)):\n",
    "    key[keys[i]] = i\n",
    "    arr = np.asarray(ma_maps_arrs[keys[i]])\n",
    "    # rescale so that the maximum value in every row is 1, to match the scaling of neurosynth data\n",
    "    transformed_arr = arr/arr.max(axis=1)[:,None]\n",
    "    dataframes[i] = pd.DataFrame(transformed_arr)\n",
    "    dataframes[i]['y'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([dataframes[0], dataframes[1]], ignore_index=True)\n",
    "for i in np.arange(2, len(keys)):\n",
    "    train_data = pd.concat([train_data, dataframes[i]], ignore_index=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data\n",
    "test_paths=['neurosynth/saccades.csv',\n",
    "      'neurosynth/tmind.csv',\n",
    "      'neurosynth/pain.csv',\n",
    "      'neurosynth/counting.csv']\n",
    "test_dataframe = {}\n",
    "for i in np.arange(0, len(keys)):\n",
    "    test_dataframe[i] = pd.read_csv(test_paths[i])\n",
    "    test_dataframe[i]['y'] = i\n",
    "test_data = pd.concat([test_dataframe[0], test_dataframe[1]], ignore_index=True)\n",
    "for i in np.arange(2, len(keys)):\n",
    "    test_data = pd.concat([test_data, test_dataframe[i]], ignore_index=True)\n",
    "test_data = test_data.drop('Unnamed: 0', 1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.unique(train_data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we include all classes for training, but this can be modified to include a subset of classes\n",
    "train_classes = unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_classes = train_data.loc[train_data['y'].isin(train_classes)].reset_index(drop=True)\n",
    "data_test_classes = test_data.loc[test_data['y'].isin(train_classes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove voxels which are zero in all classes in the training set\n",
    "zero_voxels = (data_train_classes != 0).any(axis=0).values\n",
    "sub_sampled_data_train_classes = data_train_classes.loc[:, zero_voxels]\n",
    "sub_sampled_data_test_classes = data_test_classes.loc[:, zero_voxels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sampled_data_train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sampled_data_test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to reduce dimensionality, take 100 principal components\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "pca = decomposition.PCA(n_components = 100)\n",
    "y_train = sub_sampled_data_train_classes.iloc[:,-1].values\n",
    "y_test = sub_sampled_data_test_classes.iloc[:,-1].values\n",
    "X_train_raw = sub_sampled_data_train_classes.iloc[:,:-1].values\n",
    "X_test_raw = sub_sampled_data_test_classes.iloc[:,:-1].values\n",
    "pca.fit(X_train_raw)\n",
    "# transform both training and test datasets\n",
    "X_train = pca.transform(X_train_raw)\n",
    "X_test = pca.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the variance explained by principal components\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, figsize=(6, 6))\n",
    "ax0.plot(pca.explained_variance_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "ax0.set_xticks(np.arange(0, 101, 50))\n",
    "ax0.set_xlim([0,100])\n",
    "\n",
    "ax1.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax1.set_ylabel('PCA explained variance ratio')\n",
    "ax1.set_xticks(np.arange(0, 101, 50))\n",
    "ax1.set_xlim([0,100])\n",
    "plt.show()\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit two classifiers just for fun\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# check whether the classifiers do a decent job before training\n",
    "\n",
    "# Linear SVM classifier\n",
    "clf_svc = svm.LinearSVC(max_iter = 1000000, class_weight = 'balanced')\n",
    "scores_svc = cross_val_score(clf_svc, X_train, y_train, cv=10)\n",
    "print(\"Accuracy with Linear SVM classifier: %0.2f (+/- %0.2f) \\n\" % (scores_svc.mean(), scores_svc.std() * 2))\n",
    "\n",
    "# Ridge classifier\n",
    "clf_ridge = RidgeClassifier(alpha=100, class_weight = 'balanced')\n",
    "scores_ridge = cross_val_score(clf_ridge, X_train, y_train, cv=10)\n",
    "print(\"Accuracy with Ridge classifier: %0.2f (+/- %0.2f) \\n\" % (scores_ridge.mean(), scores_ridge.std() * 2))\n",
    "\n",
    "# you can inspect the scores of individual classifiers by typing scores_svc, or scores_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving on to using the above classifiers to train on brainmap data and predicting classes of neurosynth data\n",
    "\n",
    "# Linear SVM classifier\n",
    "clf_svc = svm.LinearSVC(max_iter = 1000000, class_weight = 'balanced')\n",
    "clf_svc.fit(X_train, y_train)\n",
    "y_predict_svc = clf_svc.predict(X_test)\n",
    "svc_accuracy = len(np.where(y_test == y_predict_svc)[0])/len(y_test)\n",
    "print(\"Accuracy of SVC classifier on Neurosynth data: %0.2f \\n\" % (svc_accuracy))\n",
    "\n",
    "# Sanity check: Shuffle the test data and see whether the classifier performs at chance\n",
    "X_shuffle_test=X_test.copy()\n",
    "np.random.shuffle(X_shuffle_test)\n",
    "y_predict_svc_shuffled = clf_svc.predict(X_shuffle_test)\n",
    "svc_accuracy_shuffled = len(np.where(y_test == y_predict_svc_shuffled)[0])/len(y_test)\n",
    "print(\"Accuracy of SVC classifier on shuffled Neurosynth data: %0.2f \" % (svc_accuracy_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge classifier\n",
    "clf_ridge = RidgeClassifier(alpha=100, class_weight = 'balanced')\n",
    "clf_ridge.fit(X_train, y_train)\n",
    "y_predict_ridge = clf_ridge.predict(X_test)\n",
    "ridge_accuracy = len(np.where(y_test == y_predict_ridge)[0])/len(y_test)\n",
    "print(\"Accuracy of Ridge classifier on Neurosynth data: %0.2f \\n\" % (ridge_accuracy))\n",
    "\n",
    "# Sanity check: Shuffle the test data and see whether the classifier performs at chance\n",
    "y_predict_ridge_shuffled = clf_ridge.predict(X_shuffle_test)\n",
    "ridge_accuracy_shuffled = len(np.where(y_test == y_predict_ridge_shuffled)[0])/len(y_test)\n",
    "print(\"Accuracy of Ridge classifier on shuffled Neurosynth data: %0.2f \" % (ridge_accuracy_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
